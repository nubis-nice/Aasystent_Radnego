# Speaches STT/TTS z GPU Support - Deployment Guide

**Data**: 2026-01-16  
**Wersja**: 1.0  
**Bazuje na**: [speaches-ai/speaches](https://github.com/speaches-ai/speaches)

---

## ğŸ¯ Czym jest Speaches?

**Speaches** to OpenAI API-kompatybilny serwer dla:

- **STT (Speech-to-Text)** - powered by faster-whisper
- **TTS (Text-to-Speech)** - powered by Piper i Kokoro (ranked #1 in TTS Arena)
- **Realtime API** - streaming transcription i speech-to-speech

**Dlaczego Speaches zamiast faster-whisper-server?**

- âœ… OpenAI API compatible - drop-in replacement
- âœ… STT + TTS w jednym kontenerze
- âœ… Dynamic model loading/offloading (oszczÄ™dnoÅ›Ä‡ GPU memory)
- âœ… Streaming support
- âœ… Gradio UI out-of-the-box
- âœ… Actively maintained (34 contributors, regular releases)

---

## ğŸ“¦ Architektura w projekcie

### Obecna infrastruktura:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Aasystent Radnego Infrastructure                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  PostgreSQL (pgvector)  â†’  Port 5433                         â”‚
â”‚  Redis                  â†’  Port 6379                         â”‚
â”‚  Adminer                â†’  Port 8080                         â”‚
â”‚  Whisper (faster-whisper-server) â†’ Port 8000 (OLD)          â”‚
â”‚                                                               â”‚
â”‚  API Server             â†’  Port 3001                         â”‚
â”‚  Worker                 â†’  Background                        â”‚
â”‚  Frontend (Next.js)     â†’  Port 3000                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Po dodaniu Speaches:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Aasystent Radnego Infrastructure + Speaches                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  PostgreSQL (pgvector)  â†’  Port 5433                         â”‚
â”‚  Redis                  â†’  Port 6379                         â”‚
â”‚  Adminer                â†’  Port 8080                         â”‚
â”‚  Speaches (STT+TTS)     â†’  Port 8001 (NEW) â­ GPU REQUIRED  â”‚
â”‚                                                               â”‚
â”‚  API Server             â†’  Port 3001                         â”‚
â”‚  Worker                 â†’  Background                        â”‚
â”‚  Frontend (Next.js)     â†’  Port 3000                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Port change**: Zmieniono z 8000 â†’ **8001** Å¼eby uniknÄ…Ä‡ konfliktu z istniejÄ…cym Whisper.

---

## ğŸš€ Quick Start

### 1. Wymagania

**Hardware**:

- âœ… NVIDIA GPU z CUDA support (GTX 1060+ / RTX series)
- âœ… VRAM Requirements:
  - **4GB VRAM** (RTX 3050): medium model + int8
  - **6GB+ VRAM** (RTX 3060+): large-v3 model + float16
  - **8GB+ VRAM** (RTX 4060+): large-v3 model + float16 (recommended)
- âœ… ~10GB dysku (dla cached models)

**Software**:

- âœ… Docker + Docker Compose
- âœ… NVIDIA Docker Runtime (`nvidia-docker2`)
- âœ… NVIDIA CUDA Drivers

**SprawdÅº GPU support**:

```powershell
docker run --rm --gpus all nvidia/cuda:12.6.3-base-ubuntu24.04 nvidia-smi
```

### 2. Deployment

**Uruchomienie**:

```powershell
# Z katalogu gÅ‚Ã³wnego projektu
cd d:\Aasystent_Radnego

# Uruchom Speaches z GPU
docker-compose -f docker-compose.speaches.yaml up -d

# SprawdÅº logi
docker logs -f aasystent-speaches

# SprawdÅº status
docker ps | findstr speaches
```

**Pierwsze uruchomienie**:

- Speaches pobierze modele automatycznie (~2-5 GB)
- MoÅ¼e potrwaÄ‡ 5-10 minut w zaleÅ¼noÅ›ci od internetu
- Health check bÄ™dzie failowaÅ‚ dopÃ³ki modele siÄ™ nie zaÅ‚adujÄ…

**Weryfikacja**:

```powershell
# Health check
curl http://localhost:8001/health

# OpenAPI docs
curl http://localhost:8001/docs

# Gradio UI
# OtwÃ³rz: http://localhost:8001
```

---

## âš™ï¸ Konfiguracja

### Environment Variables

**Plik**: `speaches.env.example` â†’ skopiuj do `.env`

**Kluczowe zmienne**:

```bash
# GPU Configuration
WHISPER__INFERENCE_DEVICE=cuda
WHISPER__COMPUTE_TYPE=float16  # float16 (accuracy) vs int8 (speed)

# Model TTL (keep models loaded)
STT_MODEL_TTL=600   # 10 min
TTS_MODEL_TTL=600
VAD_MODEL_TTL=-1    # Never unload VAD

# Preload models at startup
PRELOAD_MODELS=["Systran/faster-whisper-large-v3"]

# CORS (allow frontend/API)
ALLOW_ORIGINS=["http://localhost:3000","http://localhost:3001"]

# API Key (optional)
# API_KEY=secret-key-here
```

### Compute Type Trade-offs

| Compute Type   | Speed  | Accuracy | VRAM Usage |
| -------------- | ------ | -------- | ---------- |
| `float16`      | Slow   | High     | High       |
| `int8_float16` | Medium | Medium   | Medium     |
| `int8`         | Fast   | Lower    | Low        |

**Rekomendacja**:

- **RTX 3050 (4GB)**: `int8` + `medium` model
- **RTX 3060+ (6GB+)**: `float16` + `large-v3` model
- **RTX 4060+ (8GB+)**: `float16` + `large-v3` (optimal)

---

## ğŸ”— Integracja z API

### OpenAI-Compatible API

Speaches implementuje OpenAI API, wiÄ™c istniejÄ…cy kod dziaÅ‚a out-of-the-box!

**Obecna konfiguracja** (faster-whisper-server):

```typescript
// apps/api/src/ai/ai-client-factory.ts
const client = new OpenAI({
  baseURL: "http://localhost:8000/v1",
  apiKey: "not-needed",
});
```

**Nowa konfiguracja** (Speaches):

```typescript
const client = new OpenAI({
  baseURL: "http://localhost:8001/v1", // Zmieniony port
  apiKey: "not-needed", // lub secret jeÅ›li wÅ‚Ä…czony API_KEY
});
```

**Migracja**:

1. Zatrzymaj stary Whisper: `docker stop aasystent-whisper`
2. Uruchom Speaches: `docker-compose -f docker-compose.speaches.yaml up -d`
3. ZmieÅ„ `baseURL` w konfiguracji AI provider (Settings â†’ AI Config)
4. Test transkrypcji

**WAÅ»NE**: Nie trzeba zmieniaÄ‡ kodu! OpenAI SDK dziaÅ‚a identycznie.

---

## ğŸ“Š DostÄ™pne modele

### Speech-to-Text (Whisper)

Format model ID: `Systran/faster-whisper-{size}`

| Model             | Size  | Speed     | Accuracy  | VRAM |
| ----------------- | ----- | --------- | --------- | ---- |
| `tiny`            | 39M   | Very Fast | Low       | 1GB  |
| `base`            | 74M   | Fast      | Medium    | 1GB  |
| `small`           | 244M  | Medium    | Good      | 2GB  |
| `medium`          | 769M  | Slow      | Very Good | 5GB  |
| `large-v3`        | 1550M | Very Slow | Excellent | 6GB+ |
| `distil-large-v3` | 756M  | Medium    | Excellent | 4GB  |

**Rekomendacja dla produkcji**: `Systran/faster-whisper-large-v3`

### Text-to-Speech

**Kokoro** (Ranked #1 in TTS Arena) - tylko angielski:

- `speaches-ai/Kokoro-82M-v1.0-ONNX`
- Naturalny gÅ‚os, Å›wietna jakoÅ›Ä‡
- Fast inference
- âš ï¸ **NIE obsÅ‚uguje polskiego**

---

## ğŸ‡µğŸ‡± Polski TTS - alternatywy

### 1. Edge TTS (Darmowe, online) â­ REKOMENDOWANY

Darmowy TTS od Microsoft z doskonaÅ‚Ä… obsÅ‚ugÄ… polskiego.

**Polskie gÅ‚osy**:
| GÅ‚os | Opis |
|------|------|
| `pl-PL-ZofiaNeural` | Kobieta, naturalny â­ |
| `pl-PL-MarekNeural` | MÄ™Å¼czyzna, naturalny |

**UÅ¼ycie API**:

```bash
# Test syntezy
curl -X POST http://localhost:3001/api/edge-tts/test \
  -H "Content-Type: application/json" \
  -d '{"text": "Witaj Å›wiecie!"}'

# Synteza z wyborem gÅ‚osu
curl -X POST http://localhost:3001/api/edge-tts/synthesize \
  -H "Content-Type: application/json" \
  -d '{"text": "Witaj Å›wiecie!", "voice": "pl-PL-ZofiaNeural"}'

# Lista gÅ‚osÃ³w
curl http://localhost:3001/api/edge-tts/voices
```

**Zalety**:

- âœ… Darmowe, bez limitu
- âœ… DoskonaÅ‚a jakoÅ›Ä‡ polskiego
- âœ… Nie wymaga GPU
- âœ… Wbudowane w API

**Wady**:

- âš ï¸ Wymaga poÅ‚Ä…czenia z internetem

### 2. Piper TTS (Darmowe, offline)

Lokalny neural TTS z polskimi gÅ‚osami.

**Polskie gÅ‚osy**:
| Model | Opis |
|-------|------|
| `pl_PL-gosia-medium` | Kobieta, wysoka jakoÅ›Ä‡ â­ |
| `pl_PL-darkman-medium` | MÄ™Å¼czyzna |
| `pl_PL-mc_speech-medium` | Kobieta |

**Uruchomienie**:

```bash
docker-compose -f docker-compose.piper.yaml up -d
```

**Zalety**:

- âœ… Darmowe
- âœ… Offline (prywatnoÅ›Ä‡)
- âœ… Szybkie

**Wady**:

- âš ï¸ NiÅ¼sza jakoÅ›Ä‡ niÅ¼ Edge TTS

### PorÃ³wnanie polskich TTS

| Cecha       | Edge TTS   | Piper      | Kokoro |
| ----------- | ---------- | ---------- | ------ |
| **Polski**  | â­â­â­â­â­ | â­â­â­     | âŒ     |
| **JakoÅ›Ä‡**  | DoskonaÅ‚a  | Dobra      | -      |
| **Offline** | âŒ         | âœ…         | -      |
| **GPU**     | Nie wymaga | Nie wymaga | -      |
| **Koszt**   | Darmowe    | Darmowe    | -      |

---

## ğŸ›ï¸ ZarzÄ…dzanie modelami

### Instalacja modeli

**Automatyczne przy uÅ¼yciu**:

```typescript
// Model pobierze siÄ™ automatycznie przy pierwszym uÅ¼yciu
const transcription = await client.audio.transcriptions.create({
  file: audioFile,
  model: "Systran/faster-whisper-large-v3", // Auto-download if not cached
});
```

**Manualna instalacja** (rekomendowane):

```powershell
# Pobierz model przed pierwszym uÅ¼yciem
curl -X POST http://localhost:8001/v1/models/Systran/faster-whisper-large-v3

# SprawdÅº zainstalowane modele
curl http://localhost:8001/v1/models
```

**Czas pobierania**:

- `medium`: ~1.2GB, 2-5 min
- `large-v3`: ~2.6GB, 5-10 min

### Dynamic Loading

Modele sÄ… Å‚adowane automatycznie przy pierwszym uÅ¼yciu:

```typescript
// UÅ¼yj dowolnego modelu - zostanie zaÅ‚adowany automatycznie
const transcription = await client.audio.transcriptions.create({
  file: audioFile,
  model: "Systran/faster-whisper-large-v3", // Auto-download if not cached
});
```

### Preload na starcie

```bash
# W .env lub docker-compose
PRELOAD_MODELS=["Systran/faster-whisper-large-v3","rhasspy/piper-voices"]
```

### Model TTL (Time To Live)

```bash
STT_MODEL_TTL=600   # Unload after 10 min of inactivity
STT_MODEL_TTL=-1    # Never unload (keep in VRAM)
STT_MODEL_TTL=0     # Unload immediately after use
```

**Trade-off**:

- `-1`: Fast response, high VRAM usage
- `600`: Balanced (unload after 10 min idle)
- `0`: Low VRAM, slow (load model kaÅ¼dy request)

---

## ğŸ§ª Testing

### 1. Health Check

```bash
curl http://localhost:8001/health
# Response: {"status":"ok"}
```

### 2. Transcription Test

```bash
curl -X POST http://localhost:8001/v1/audio/transcriptions \
  -H "Content-Type: multipart/form-data" \
  -F "file=@test_audio.mp3" \
  -F "model=Systran/faster-whisper-large-v3" \
  -F "language=pl"
```

### 3. TTS Test

```bash
curl -X POST http://localhost:8001/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d '{
    "model": "kokoro",
    "input": "Witaj Å›wiecie!",
    "voice": "af_sky"
  }' \
  --output speech.mp3
```

### 4. Gradio UI

OtwÃ³rz http://localhost:8001 w przeglÄ…darce - interfejs do testowania STT/TTS.

---

## ğŸ“ˆ Performance

### Transcription Benchmarks (large-v3, RTX 3080)

| Audio Length | Time (float16) | Time (int8) | Speedup |
| ------------ | -------------- | ----------- | ------- |
| 1 min        | 6s             | 3s          | 2x      |
| 10 min       | 45s            | 22s         | 2x      |
| 30 min       | 2m 15s         | 1m 10s      | ~2x     |
| 60 min       | 4m 30s         | 2m 20s      | ~2x     |

**Note**: int8 jest ~2x szybszy ale nieco niÅ¼sza jakoÅ›Ä‡.

### Memory Usage

| Model              | VRAM (idle) | VRAM (inferencing) |
| ------------------ | ----------- | ------------------ |
| large-v3 (float16) | ~3GB        | ~6GB               |
| large-v3 (int8)    | ~1.5GB      | ~3GB               |
| distil-large-v3    | ~2GB        | ~4GB               |

---

## ğŸ”§ Troubleshooting

### Problem: "no CUDA-capable device is detected"

**RozwiÄ…zanie**:

```powershell
# SprawdÅº NVIDIA Docker runtime
docker run --rm --gpus all nvidia/cuda:12.6.3-base-ubuntu24.04 nvidia-smi

# JeÅ›li error, zainstaluj nvidia-docker2:
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
```

### Problem: "Model not found"

**RozwiÄ…zanie**:

- SprawdÅº format model ID: `Systran/faster-whisper-{size}`
- Poczekaj na auto-download (pierwsze uÅ¼ycie)
- SprawdÅº logi: `docker logs aasystent-speaches`

### Problem: Out of Memory (CUDA OOM)

**RozwiÄ…zanie**:

1. UÅ¼yj mniejszego modelu: `large-v3` â†’ `distil-large-v3` â†’ `medium`
2. ZmieÅ„ compute type: `float16` â†’ `int8`
3. Zmniejsz batch size (jeÅ›li uÅ¼ywasz batch processing)
4. WÅ‚Ä…cz aggressive model unloading: `STT_MODEL_TTL=0`

### Problem: Slow transcription

**RozwiÄ…zanie**:

1. SprawdÅº czy uÅ¼ywa GPU: `docker stats aasystent-speaches` (powinien uÅ¼ywaÄ‡ GPU)
2. ZmieÅ„ na int8: `WHISPER__COMPUTE_TYPE=int8`
3. UÅ¼yj mniejszego modelu: `distil-large-v3`
4. Preload model: `PRELOAD_MODELS=["Systran/faster-whisper-large-v3"]`

### Problem: Port 8001 already in use

**RozwiÄ…zanie**:

```yaml
# W docker-compose.speaches.yaml zmieÅ„ port:
ports:
  - "8002:8000" # Lub inny wolny port
```

---

## ğŸ“š Dokumentacja i Zasoby

**Oficjalna dokumentacja**:

- [Speaches Docs](https://speaches.ai/)
- [GitHub repo](https://github.com/speaches-ai/speaches)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference/audio)

**Modele**:

- [Faster Whisper](https://github.com/SYSTRAN/faster-whisper)
- [Piper TTS](https://github.com/rhasspy/piper)
- [Kokoro TTS](https://huggingface.co/hexgrad/Kokoro-82M)

**Related Docs w projekcie**:

- `docs/FIX_STT_TIMEOUT_PROBLEM.md` - timeout handling
- `docs/MVP_AUDIO_CHUNKING.md` - chunked transcription
- `docs/DESIGN_AUDIO_CHUNKING_SYSTEM.md` - full design

---

## ğŸš€ Production Checklist

### Pre-deployment:

- [ ] GPU drivers zainstalowane (`nvidia-smi` dziaÅ‚a)
- [ ] NVIDIA Docker runtime zainstalowany
- [ ] Port 8001 wolny (lub zmieniony w docker-compose)
- [ ] Min 10GB wolnego miejsca na dysku (models cache)
- [ ] Min 6GB VRAM dla large-v3

### Deployment:

- [ ] Docker Compose up: `docker-compose -f docker-compose.speaches.yaml up -d`
- [ ] Health check OK: `curl http://localhost:8001/health`
- [ ] Model preload complete (sprawdÅº logi)
- [ ] Test transcription z krÃ³tkim plikiem
- [ ] Test z 10+ min plikiem (dla pewnoÅ›ci)

### Integration:

- [ ] Zmiana baseURL w AI provider config (Settings)
- [ ] Test z real YouTube video
- [ ] Weryfikacja audio preprocessing dziaÅ‚a
- [ ] Sprawdzenie chunked transcription (dla 30+ min)
- [ ] Monitoring GPU usage (`nvidia-smi`)

### Monitoring:

- [ ] Health checks w Uptime Kuma/Grafana
- [ ] Disk space dla models cache
- [ ] GPU memory usage alerts
- [ ] Transcription latency metrics

---

## ğŸ”„ Migration from faster-whisper-server

### Step-by-step:

**1. Backup (optional)**:

```powershell
docker commit aasystent-whisper backup-whisper-$(Get-Date -Format 'yyyyMMdd')
```

**2. Stop old Whisper**:

```powershell
docker stop aasystent-whisper
# Nie usuwaj jeszcze, na wypadek rollback
```

**3. Start Speaches**:

```powershell
docker-compose -f docker-compose.speaches.yaml up -d
```

**4. Update AI Config**:

- IdÅº do Settings â†’ AI Configuration
- STT Provider â†’ zmieÅ„ base URL: `http://localhost:8000/v1` â†’ `http://localhost:8001/v1`
- Save

**5. Test**:

- UtwÃ³rz test transcription job
- SprawdÅº logi Worker: `cd apps/worker && npm run dev`
- Zweryfikuj output

**6. Cleanup (po 24h stabilnego dziaÅ‚ania)**:

```powershell
docker rm aasystent-whisper
docker volume rm aasystent-whisper-models  # jeÅ›li istnieje
```

**Rollback (if needed)**:

```powershell
docker-compose -f docker-compose.speaches.yaml down
docker start aasystent-whisper
# PrzywrÃ³Ä‡ old baseURL w AI Config
```

---

## ğŸ’¡ Tips & Best Practices

### 1. Model Selection

- **Development**: `distil-large-v3` (fast, good quality)
- **Production**: `large-v3` (best accuracy)
- **Low VRAM**: `medium` lub `small`

### 2. Compute Type

- **Production**: `float16` (best quality)
- **High throughput**: `int8` (2x faster, 90% quality)

### 3. Model TTL

- **Low traffic**: `STT_MODEL_TTL=300` (5 min)
- **High traffic**: `STT_MODEL_TTL=-1` (never unload)
- **Multi-user**: `STT_MODEL_TTL=600` (10 min)

### 4. Preloading

Preload najczÄ™Å›ciej uÅ¼ywane modele:

```bash
PRELOAD_MODELS=["Systran/faster-whisper-large-v3"]
```

### 5. Security

W produkcji wÅ‚Ä…cz API key:

```bash
API_KEY=random-secret-key-$(uuidgen)
```

### 6. Monitoring

Obserwuj:

- GPU memory: `nvidia-smi dmon`
- Docker stats: `docker stats aasystent-speaches`
- Disk space: modele cached w `/home/ubuntu/.cache/huggingface/hub`

---

## ğŸ“Š Comparison: faster-whisper-server vs Speaches

| Feature                | faster-whisper-server | Speaches                    |
| ---------------------- | --------------------- | --------------------------- |
| **STT**                | âœ… Whisper            | âœ… Whisper (faster-whisper) |
| **TTS**                | âŒ                    | âœ… Piper + Kokoro           |
| **API**                | Custom                | âœ… OpenAI Compatible        |
| **Streaming**          | âŒ                    | âœ… SSE                      |
| **Dynamic loading**    | âŒ (manual)           | âœ… Automatic                |
| **Web UI**             | âŒ                    | âœ… Gradio                   |
| **Realtime API**       | âŒ                    | âœ…                          |
| **Active development** | Moderate              | âœ… Very active              |
| **GPU Support**        | âœ…                    | âœ…                          |
| **Multi-model**        | Single at a time      | âœ… Dynamic switching        |

**Verdict**: Speaches jest bardziej feature-rich i aktywnie rozwijany. Idealny dla AI voice applications.

---

**Status**: âœ… **Ready for deployment**  
**Next**: Test z real YouTube videos i migracja z faster-whisper-server

**Estimated migration time**: 30 min
