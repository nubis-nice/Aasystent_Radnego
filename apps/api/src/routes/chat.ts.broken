import { FastifyPluginAsync } from "fastify";
import { ZodError } from "zod";
import OpenAI from "openai";
import {
  ChatRequestSchema,
  buildSystemPrompt,
  type RAGContext,
  type SystemPromptContext,
} from "@aasystent-radnego/shared";
import { createClient } from "@supabase/supabase-js";
import {
  optimizeContext,
  type DocumentContext,
  type ConversationMessage,
} from "../services/context-compressor.js";
import { DocumentQueryService } from "../services/document-query-service.js";
import { getLLMClient, getEmbeddingsClient, getAIConfig } from "../ai/index.js";

/* eslint-disable no-undef */
declare const Buffer: typeof globalThis.Buffer;

const supabaseUrl = process.env.SUPABASE_URL!;
const supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY!;
const supabase = createClient(supabaseUrl, supabaseServiceKey);

/**
 * Generuje embedding dla d≈Çugiego tekstu u≈ºywajƒÖc batch processing
 * Dzieli tekst na chunki, generuje embedding dla ka≈ºdego, agreguje wyniki
 *
 * Strategia agregacji: ≈õrednia wa≈ºona wektor√≥w (weighted by chunk length)
 * To zachowuje semantykƒô ca≈Çego tekstu zamiast traciƒá informacje przez obcinanie
 */
async function generateBatchEmbedding(
  client: OpenAI,
  text: string,
  model: string,
  maxChunkChars: number = 18000
): Promise<number[]> {
  // Je≈õli tekst mie≈õci siƒô w limicie - pojedyncze wywo≈Çanie
  if (text.length <= maxChunkChars) {
    console.log(`[Embedding] Single chunk: ${text.length} chars`);
    const response = await client.embeddings.create({
      model,
      input: text,
    });
    return response.data[0].embedding;
  }

  // Podziel tekst na chunki z overlap dla zachowania kontekstu
  const overlap = 500; // 500 znak√≥w overlap miƒôdzy chunkami
  const chunks: string[] = [];
  let start = 0;

  while (start < text.length) {
    const end = Math.min(start + maxChunkChars, text.length);
    let chunk = text.slice(start, end);

    // Znajd≈∫ koniec zdania dla naturalnego podzia≈Çu
    if (end < text.length) {
      const lastPeriod = chunk.lastIndexOf(". ");
      const lastNewline = chunk.lastIndexOf("\n");
      const breakPoint = Math.max(lastPeriod, lastNewline);

      if (breakPoint > maxChunkChars * 0.7) {
        chunk = chunk.slice(0, breakPoint + 1);
        start += breakPoint + 1 - overlap;
      } else {
        start = end - overlap;
      }
    } else {
      start = end;
    }

    if (chunk.trim().length > 100) {
      chunks.push(chunk.trim());
    }
  }

  console.log(
    `[Embedding] Batch processing: ${chunks.length} chunks from ${text.length} chars`
  );

  // Generuj embeddingi dla wszystkich chunk√≥w (batch API)
  const response = await client.embeddings.create({
    model,
    input: chunks,
  });

  // Agregacja: ≈õrednia wa≈ºona wektor√≥w wed≈Çug d≈Çugo≈õci chunk√≥w
  const embeddings = response.data.map((d) => d.embedding);
  const weights = chunks.map((c) => c.length);
  const totalWeight = weights.reduce((sum, w) => sum + w, 0);

  // Oblicz ≈õredniƒÖ wa≈ºonƒÖ dla ka≈ºdego wymiaru wektora
  const dimensions = embeddings[0].length;
  const aggregatedEmbedding: number[] = new Array(dimensions).fill(0);

  for (let dim = 0; dim < dimensions; dim++) {
    let weightedSum = 0;
    for (let i = 0; i < embeddings.length; i++) {
      weightedSum += embeddings[i][dim] * weights[i];
    }
    aggregatedEmbedding[dim] = weightedSum / totalWeight;
  }

  // Normalizacja wektora (L2 norm) - wa≈ºne dla similarity search
  const norm = Math.sqrt(
    aggregatedEmbedding.reduce((sum, val) => sum + val * val, 0)
  );
  const normalizedEmbedding = aggregatedEmbedding.map((val) => val / norm);

  console.log(
    `[Embedding] Aggregated ${chunks.length} embeddings into single vector`
  );

  return normalizedEmbedding;
}

export const chatRoutes: FastifyPluginAsync = async (fastify) => {
  // POST /api/chat/message - Wy≈õlij wiadomo≈õƒá do AI
  fastify.post<{
    Body: {
      message: string;
      conversationId?: string;
      includeDocuments?: boolean;
      includeMunicipalData?: boolean;
      temperature?: number;
    };
  }>("/chat/message", async (request, reply) => {
    try {
      // Walidacja
      const validatedData = ChatRequestSchema.parse(request.body);
      const {
        message,
        conversationId,
        includeDocuments,
        includeMunicipalData,
        temperature,
      } = validatedData;

      // Pobierz u≈ºytkownika z headera (zak≈Çadamy ≈ºe auth middleware dodaje user_id)
      const userId = request.headers["x-user-id"] as string;
      if (!userId) {
        return reply.status(401).send({ error: "Unauthorized" });
      }

      // Pobierz profil u≈ºytkownika
      const { data: profile } = await supabase
        .from("user_profiles")
        .select("*")
        .eq("user_id", userId)
        .single();

      // Pobierz klient√≥w AI z fabryki (automatyczna konfiguracja, cache, fallback)
      let openai: OpenAI;
      let embeddingsClient: OpenAI;
      let model: string;
      let embeddingModel: string;

      try {
        openai = await getLLMClient(userId);
        embeddingsClient = await getEmbeddingsClient(userId);

        const llmConfig = await getAIConfig(userId, "llm");
        const embConfig = await getAIConfig(userId, "embeddings");

        model = llmConfig.modelName;
        embeddingModel = embConfig.modelName;

        console.log(
          `[Chat] Using LLM: provider=${llmConfig.provider}, model=${model}`
        );
        console.log(
          `[Chat] Using Embeddings: provider=${embConfig.provider}, model=${embeddingModel}`
        );
      } catch (configError) {
        console.error("[Chat] AI config error:", configError);
        return reply.status(400).send({
          error:
            "Brak konfiguracji API. Przejd≈∫ do Ustawienia ‚Üí Konfiguracja API i dodaj klucz API.",
        });
      }

      // Pobierz lub utw√≥rz konwersacjƒô
      let currentConversationId = conversationId;

      if (!currentConversationId) {
        // Utw√≥rz nowƒÖ konwersacjƒô
        const { data: newConversation, error: convError } = await supabase
          .from("conversations")
          .insert({
            user_id: userId,
            title: message.substring(0, 100),
          })
          .select()
          .single();

        if (convError || !newConversation) {
          throw new Error("Failed to create conversation");
        }

        currentConversationId = newConversation.id;
      }

      // Zapisz wiadomo≈õƒá u≈ºytkownika
      await supabase.from("messages").insert({
        conversation_id: currentConversationId,
        role: "user",
        content: message,
      });

      // Pobierz historiƒô konwersacji (ostatnie 10 wiadomo≈õci)
      const { data: history } = await supabase
        .from("messages")
        .select("role, content")
        .eq("conversation_id", currentConversationId)
        .order("created_at", { ascending: true })
        .limit(10);

      // ========================================================================
      // PHASE 1: WYKRYWANIE DOKUMENT√ìW W WIADOMO≈öCI
      // Zamiast przekazywaƒá pe≈ÇnƒÖ tre≈õƒá, wykrywamy ID/nazwƒô i szukamy w RAG
      // ========================================================================

      const documentQueryService = new DocumentQueryService(userId);
      await documentQueryService.initialize();

      const documentQuery = await documentQueryService.queryDocuments(message);

      // Je≈õli wykryto dokument i wymaga potwierdzenia - zwr√≥ƒá pytanie
      if (
        documentQuery.found &&
        documentQuery.needsConfirmation &&
        documentQuery.confirmationMessage
      ) {
        console.log(`[Chat] Document detected, asking for confirmation`);

        // Zapisz odpowied≈∫ asystenta z pytaniem o potwierdzenie
        const confirmationResponse = documentQuery.confirmationMessage;

        const { data: confirmationMessage } = await supabase
          .from("messages")
          .insert({
            conversation_id: currentConversationId,
            role: "assistant",
            content: confirmationResponse,
          })
          .select()
          .single();

        return reply.send({
          conversationId: currentConversationId,
          message: confirmationMessage || {
            id: `temp-${Date.now()}`,
            conversationId: currentConversationId,
            role: "assistant",
            content: confirmationResponse,
            citations: [],
            createdAt: new Date().toISOString(),
          },
          citations: [],
          documentQuery: {
            found: true,
            matches: documentQuery.matches,
            needsConfirmation: true,
          },
        });
      }

      // Je≈õli dokument znaleziony bez potrzeby potwierdzenia - pobierz kontekst
      let documentContext = null;
      if (
        documentQuery.found &&
        !documentQuery.needsConfirmation &&
        documentQuery.matches.length > 0
      ) {
        const primaryDoc = documentQuery.matches[0];
        if (primaryDoc) {
          console.log(
            `[Chat] Document found by ID: ${primaryDoc.id}, fetching context`
          );
          documentContext = await documentQueryService.getDocumentContext(
            primaryDoc.id,
            message
          );
        }
      }

      // Przygotuj kontekst RAG
      let ragContext: RAGContext | undefined;

      if (includeDocuments || includeMunicipalData) {
        // U≈ºyj klienta embeddings z fabryki (ju≈º skonfigurowany)
        // Batch embedding dla d≈Çugich wiadomo≈õci
        // text-embedding-3-small: limit 8192 token√≥w ‚âà 20000 znak√≥w
        const maxChunkChars = 18000;
        const queryEmbedding = await generateBatchEmbedding(
          embeddingsClient,
          message,
          embeddingModel,
          maxChunkChars
        );

        ragContext = {
          documents: [],
          municipalData: [],
        };

        // Wyszukaj w przetworzonych dokumentach (≈∫r√≥d≈Ça danych)
        if (includeDocuments || includeMunicipalData) {
            console.log("[Chat] Searching documents for user:", userId);
            console.log(
              "[Chat] Embedding generated, length:",
              queryEmbedding.length
            );

            // Diagnostyka: ile dokument√≥w ma embeddingi
            const { count: totalDocs } = await supabase
              .from("processed_documents")
              .select("*", { count: "exact", head: true })
              .eq("user_id", userId);

            const { count: docsWithEmbedding } = await supabase
              .from("processed_documents")
              .select("*", { count: "exact", head: true })
              .eq("user_id", userId)
              .not("embedding", "is", null);

            console.log("[Chat] Documents stats:", {
              total: totalDocs,
              withEmbedding: docsWithEmbedding,
            });

            const { data: relevantDocs, error: searchError } =
              await supabase.rpc("search_processed_documents", {
                query_embedding: queryEmbedding,
                match_threshold: 0.3, // Obni≈ºony pr√≥g dla lepszych wynik√≥w
                match_count: 10,
                filter_user_id: userId,
                filter_types: null, // Wszystkie typy dokument√≥w
              });

            console.log("[Chat] Search result:", {
              found: relevantDocs?.length || 0,
              error: searchError?.message || null,
            });

            if (relevantDocs && relevantDocs.length > 0) {
              // Log document types for debugging
              const docTypes = relevantDocs.map((d: any) => d.document_type);
              console.log("[Chat] Found document types:", docTypes);

              // Dodaj wszystkie znalezione dokumenty do kontekstu
              relevantDocs.forEach((doc: any) => {
                const docData = {
                  id: doc.id,
                  title: doc.title,
                  content: doc.content,
                  relevanceScore: doc.similarity,
                  metadata: {
                    documentType: doc.document_type,
                    publishDate: doc.publish_date,
                    sourceUrl: doc.source_url,
                  },
                };

                // Zawsze dodaj do dokument√≥w je≈õli w≈ÇƒÖczone
                if (includeDocuments) {
                  ragContext!.documents.push(docData);
                }

                // Dodaj do danych gminnych je≈õli to odpowiedni typ
                if (
                  includeMunicipalData &&
                  [
                    "news",
                    "resolution",
                    "protocol",
                    "announcement",
                    "article",
                  ].includes(doc.document_type)
                ) {
                  ragContext!.municipalData.push({
                    id: doc.id,
                    title: doc.title,
                    content: doc.content || "",
                    dataType: doc.document_type,
                    relevanceScore: doc.similarity,
                  });
                }
              });

              console.log("[Chat] RAG context built:", {
                documents: ragContext!.documents.length,
                municipalData: ragContext!.municipalData.length,
              });
            }
          }
        }
      }

      // Zbuduj system prompt
      const systemPromptContext: SystemPromptContext = {
        municipalityName: profile?.municipality_name,
        municipalityType: profile?.municipality_type,
        userName: profile?.full_name,
        userPosition: profile?.position,
      };

      const systemPrompt = buildSystemPrompt(systemPromptContext);

      // ========================================================================
      // CONTEXT COMPRESSION - optymalizacja kontekstu dla oszczƒôdno≈õci token√≥w
      // ========================================================================

      // Przygotuj dokumenty do kompresji
      // WA≈ªNE: Je≈õli mamy documentContext z wykrytego dokumentu, u≈ºyj chunk√≥w zamiast pe≈Çnej tre≈õci
      let ragDocuments: DocumentContext[] = [];

      if (documentContext && documentContext.relevantChunks.length > 0) {
        // U≈ºyj chunk√≥w z wykrytego dokumentu (nie pe≈Çnej tre≈õci!)
        console.log(
          `[Chat] Using ${documentContext.relevantChunks.length} chunks from detected document`
        );
        ragDocuments = [
          {
            id: documentContext.documentId,
            title: documentContext.title,
            content: documentContext.relevantChunks
              .map((c) => c.content)
              .join("\n\n---\n\n"),
            relevanceScore: 1.0,
            metadata: { documentType: documentContext.documentType },
          },
        ];

        // Dodaj powiƒÖzane dokumenty i za≈ÇƒÖczniki (tylko metadane)
        documentContext.relatedDocuments.forEach((doc) => {
          ragDocuments.push({
            id: doc.id,
            title: `[PowiƒÖzany] ${doc.title}`,
            content: doc.summary || `Dokument typu: ${doc.documentType}`,
            relevanceScore: doc.similarity,
            metadata: { documentType: doc.documentType, isRelated: true },
          });
        });

        documentContext.attachments.forEach((att) => {
          ragDocuments.push({
            id: att.id,
            title: `[Za≈ÇƒÖcznik] ${att.title}`,
            content: att.summary || `Za≈ÇƒÖcznik typu: ${att.documentType}`,
            relevanceScore: 1.0,
            metadata: { documentType: att.documentType, isAttachment: true },
          });
        });
      } else {
        // Standardowy RAG z wyszukiwania semantycznego
        ragDocuments =
          ragContext?.documents.map((doc) => ({
            id: doc.id,
            title: doc.title,
            content: doc.content,
            relevanceScore: doc.relevanceScore,
            metadata: doc.metadata,
          })) || [];
      }

      const ragMunicipalData: DocumentContext[] =
        ragContext?.municipalData.map((item) => ({
          id: item.id,
          title: item.title,
          content: item.content,
          relevanceScore: item.relevanceScore,
          metadata: { dataType: item.dataType },
        })) || [];

      // Przygotuj historiƒô konwersacji
      const conversationHistory: ConversationMessage[] = (history || []).map(
        (msg: { role: string; content: string }) => ({
          role: msg.role as "user" | "assistant" | "system",
          content: msg.content,
        })
      );

      // Optymalizuj kontekst z kompresjƒÖ
      const optimized = optimizeContext(
        systemPrompt,
        ragDocuments,
        ragMunicipalData,
        conversationHistory,
        message,
        model,
        2000 // max completion tokens
      );

      // Log oszczƒôdno≈õci
      console.log(`[Chat] Context optimization:`, {
        originalTokens: optimized.savings.originalTokens,
        compressedTokens: optimized.savings.compressedTokens,
        savedTokens: optimized.savings.savedTokens,
        savingsPercent: `${optimized.savings.savingsPercent}%`,
        model,
      });

      // Przygotuj kontekst dla AI
      const messages: OpenAI.Chat.ChatCompletionMessageParam[] = [
        {
          role: "system",
          content: optimized.systemPrompt,
        },
      ];

      // Dodaj skompresowany kontekst RAG je≈õli dostƒôpny
      if (optimized.ragContextMessage) {
        messages.push({
          role: "system",
          content: optimized.ragContextMessage,
        });
      }

      // Dodaj skompresowanƒÖ historiƒô konwersacji
      if (optimized.historyMessages.length > 0) {
        optimized.historyMessages.forEach((msg) => {
          messages.push({
            role: msg.role as "user" | "assistant" | "system",
            content: msg.content,
          });
        });
      }

      // Dodaj aktualnƒÖ wiadomo≈õƒá u≈ºytkownika
      messages.push({
        role: "user",
        content: optimized.userMessage,
      });

      // Wywo≈Çaj AI z konfiguracjƒÖ u≈ºytkownika
      const clientConfig: any = {
        apiKey: apiKey,
        baseURL: baseUrl,
      };

      // Google Gemini native API u≈ºywa x-goog-api-key
      if (provider === "google" && baseUrl && !baseUrl.includes("/openai")) {
        clientConfig.defaultHeaders = {
          "x-goog-api-key": apiKey,
        };
        clientConfig.apiKey = "dummy"; // OpenAI client wymaga apiKey, ale nie jest u≈ºywany
      }

      const openai = new OpenAI(clientConfig);

      // U≈ºyj max_completion_tokens dla nowych modeli OpenAI, max_tokens dla starszych
      const completionParams: any = {
        model: model,
        messages,
        temperature: temperature || 0.7,
      };

      // Nowe modele OpenAI u≈ºywajƒÖ max_completion_tokens
      // Wszystkie modele gpt-4o* (gpt-4o, gpt-4o-mini) i gpt-4-turbo
      if (
        model.includes("gpt-4o") ||
        model.includes("gpt-4-turbo") ||
        model.includes("o1")
      ) {
        completionParams.max_completion_tokens = 2000;
      } else {
        completionParams.max_tokens = 2000;
      }

      // Log przed wywo≈Çaniem API
      console.log(
        `[Chat] Calling LLM with ${messages.length} messages, model: ${model}`
      );
      console.log(`[Chat] Estimated total tokens: ${optimized.totalTokens}`);

      const completion = await openai.chat.completions.create(completionParams);

      const aiResponse =
        completion.choices[0]?.message?.content ||
        "Przepraszam, nie mogƒô wygenerowaƒá odpowiedzi.";

      // Przygotuj cytaty
      const citations = [];
      if (ragContext) {
        ragContext.documents.forEach((doc) => {
          citations.push({
            documentId: doc.id,
            documentTitle: doc.title || "Bez tytu≈Çu",
            text: (doc.content || "").substring(0, 200) + "...",
            relevanceScore: doc.relevanceScore,
          });
        });

        ragContext.municipalData.forEach((item) => {
          citations.push({
            documentTitle: `${item.title || "Bez tytu≈Çu"} (${item.dataType})`,
            text: (item.content || "").substring(0, 200) + "...",
            relevanceScore: item.relevanceScore,
          });
        });
      }

      // Zapisz odpowied≈∫ AI
      const { data: aiMessage } = await supabase
        .from("messages")
        .insert({
          conversation_id: currentConversationId,
          role: "assistant",
          content: aiResponse,
          citations: citations,
        })
        .select()
        .single();

      // Zwr√≥ƒá odpowied≈∫ - zawsze z message zawierajƒÖcym id
      return reply.send({
        conversationId: currentConversationId,
        message: aiMessage || {
          id: `temp-${Date.now()}`,
          conversationId: currentConversationId,
          role: "assistant",
          content: aiResponse,
          citations: citations,
          createdAt: new Date().toISOString(),
        },
        relatedDocuments: ragContext?.documents.map((doc) => ({
          id: doc.id,
          title: doc.title,
          relevanceScore: doc.relevanceScore,
        })),
      });
    } catch (error) {
      if (error instanceof ZodError) {
        return reply
          .status(400)
          .send({ error: "Validation error", details: error.issues });
      }

      const errorMessage =
        error instanceof Error ? error.message : String(error);
      const errorStack = error instanceof Error ? error.stack : undefined;
      console.error("[Chat] Error:", errorMessage);
      if (errorStack) {
        console.error("[Chat] Stack:", errorStack);
      }

      // Obs≈Çuga b≈Çƒôdu wyczerpania limitu OpenAI (429)
      if (
        errorMessage.includes("exceeded your current quota") ||
        errorMessage.includes("insufficient_quota") ||
        errorMessage.includes("429")
      ) {
        return reply.status(402).send({
          error: "QUOTA_EXCEEDED",
          message:
            "Wyczerpano limit API OpenAI. Do≈Çaduj konto lub zmie≈Ñ klucz API.",
          details:
            "Tw√≥j klucz API OpenAI wyczerpa≈Ç dostƒôpny limit. Aby kontynuowaƒá korzystanie z Asystenta AI, musisz do≈Çadowaƒá konto OpenAI.",
          billingUrl: "https://platform.openai.com/account/billing/overview",
          settingsUrl: "/settings/api",
        });
      }

      // Obs≈Çuga b≈Çƒôdu nieprawid≈Çowego klucza API
      if (
        errorMessage.includes("invalid_api_key") ||
        errorMessage.includes("Incorrect API key")
      ) {
        return reply.status(401).send({
          error: "INVALID_API_KEY",
          message: "Nieprawid≈Çowy klucz API OpenAI.",
          details: "Sprawd≈∫ poprawno≈õƒá klucza API w ustawieniach.",
          settingsUrl: "/settings/api",
        });
      }

      return reply.status(500).send({
        error: "CHAT_ERROR",
        message: "Nie uda≈Ço siƒô przetworzyƒá wiadomo≈õci.",
        details: error instanceof Error ? error.message : "Unknown error",
      });
    }
  });

  // GET /api/chat/conversations - Pobierz listƒô konwersacji z ostatniƒÖ wiadomo≈õciƒÖ
  fastify.get("/chat/conversations", async (request, reply) => {
    try {
      const userId = request.headers["x-user-id"] as string;
      if (!userId) {
        return reply.status(401).send({ error: "Unauthorized" });
      }

      const { data: conversations, error } = await supabase
        .from("conversations")
        .select("*")
        .eq("user_id", userId)
        .order("updated_at", { ascending: false })
        .limit(50);

      if (error) {
        throw error;
      }

      // Dla ka≈ºdej konwersacji pobierz ostatniƒÖ wiadomo≈õƒá i liczbƒô wiadomo≈õci
      const conversationsWithDetails = await Promise.all(
        (conversations || []).map(async (conv) => {
          // Pobierz ostatniƒÖ wiadomo≈õƒá
          const { data: lastMessage } = await supabase
            .from("messages")
            .select("content, created_at, role")
            .eq("conversation_id", conv.id)
            .order("created_at", { ascending: false })
            .limit(1)
            .single();

          // Policz wiadomo≈õci
          const { count } = await supabase
            .from("messages")
            .select("*", { count: "exact", head: true })
            .eq("conversation_id", conv.id);

          return {
            ...conv,
            lastMessage: lastMessage?.content || null,
            lastMessageAt: lastMessage?.created_at || conv.updated_at,
            lastMessageRole: lastMessage?.role || null,
            messageCount: count || 0,
          };
        })
      );

      return reply.send({ conversations: conversationsWithDetails });
    } catch (error) {
      fastify.log.error("Error fetching conversations:", error);
      return reply.status(500).send({ error: "Failed to fetch conversations" });
    }
  });

  // GET /api/chat/conversation/:id - Pobierz konwersacjƒô z wiadomo≈õciami
  fastify.get<{
    Params: { id: string };
  }>("/chat/conversation/:id", async (request, reply) => {
    try {
      const userId = request.headers["x-user-id"] as string;
      if (!userId) {
        return reply.status(401).send({ error: "Unauthorized" });
      }

      const { id } = request.params;

      // Pobierz konwersacjƒô
      const { data: conversation, error: convError } = await supabase
        .from("conversations")
        .select("*")
        .eq("id", id)
        .eq("user_id", userId)
        .single();

      if (convError || !conversation) {
        return reply.status(404).send({ error: "Conversation not found" });
      }

      // Pobierz wiadomo≈õci
      const { data: messages, error: msgError } = await supabase
        .from("messages")
        .select("*")
        .eq("conversation_id", id)
        .order("created_at", { ascending: true });

      if (msgError) {
        throw msgError;
      }

      return reply.send({
        conversation: {
          ...conversation,
          messages,
        },
      });
    } catch (error) {
      fastify.log.error("Error fetching conversation:", error);
      return reply.status(500).send({ error: "Failed to fetch conversation" });
    }
  });

  // DELETE /api/chat/conversation/:id - Usu≈Ñ konwersacjƒô
  fastify.delete<{
    Params: { id: string };
  }>("/chat/conversation/:id", async (request, reply) => {
    try {
      const userId = request.headers["x-user-id"] as string;
      if (!userId) {
        console.log("[DELETE] Unauthorized - no user ID");
        return reply.status(401).send({ error: "Unauthorized" });
      }

      const { id } = request.params;
      console.log("[DELETE] Deleting conversation:", { id, userId });

      const { error } = await supabase
        .from("conversations")
        .delete()
        .eq("id", id)
        .eq("user_id", userId);

      if (error) {
        console.error("[DELETE] Supabase error:", error);
        throw error;
      }

      console.log("[DELETE] Successfully deleted conversation:", id);
      return reply.send({ success: true });
    } catch (error) {
      fastify.log.error("Error deleting conversation:", error);
      console.error("[DELETE] Full error:", error);
      return reply.status(500).send({ error: "Failed to delete conversation" });
    }
  });

  // POST /api/chat/create-document - Utw√≥rz dokument na bazie analizy czatu
  fastify.post<{
    Body: {
      title: string;
      content: string;
      documentType: string;
      summary?: string;
      keywords?: string[];
      conversationId?: string;
    };
  }>("/chat/create-document", async (request, reply) => {
    try {
      const userId = request.headers["x-user-id"] as string;
      if (!userId) {
        return reply.status(401).send({ error: "Unauthorized" });
      }

      const {
        title,
        content,
        documentType,
        summary,
        keywords,
        conversationId,
      } = request.body;

      // Walidacja
      if (!title || !content || !documentType) {
        return reply.status(400).send({
          error: "Missing required fields: title, content, documentType",
        });
      }

      // Pobierz konfiguracjƒô OpenAI u≈ºytkownika
      const { data: apiConfig } = await supabase
        .from("api_configurations")
        .select("*")
        .eq("user_id", userId)
        .eq("is_active", true)
        .eq("is_default", true)
        .eq("provider", "openai")
        .single();

      let openaiApiKey = process.env.OPENAI_API_KEY;
      let openaiBaseUrl: string | undefined = undefined;

      if (apiConfig) {
        openaiApiKey = Buffer.from(
          apiConfig.api_key_encrypted,
          "base64"
        ).toString("utf-8");
        openaiBaseUrl = apiConfig.base_url || undefined;
      }

      if (!openaiApiKey) {
        return reply.status(400).send({
          error: "Brak konfiguracji OpenAI. Dodaj klucz API w ustawieniach.",
        });
      }

      // Generuj embedding dla nowego dokumentu
      const openai = new OpenAI({
        apiKey: openaiApiKey,
        baseURL: openaiBaseUrl,
      });

      const embeddingResponse = await openai.embeddings.create({
        model: "text-embedding-3-small",
        input: content,
      });

      const embedding = embeddingResponse.data[0].embedding;

      // Zapisz dokument do bazy
      const { data: newDocument, error: insertError } = await supabase
        .from("processed_documents")
        .insert({
          user_id: userId,
          document_type: documentType,
          title: title,
          content: content,
          summary: summary || content.substring(0, 200) + "...",
          keywords: keywords || [],
          embedding: embedding,
          source_url: conversationId
            ? `/chat/conversation/${conversationId}`
            : null,
          metadata: {
            created_by: "ai_assistant",
            conversation_id: conversationId,
            created_at: new Date().toISOString(),
          },
        })
        .select()
        .single();

      if (insertError || !newDocument) {
        throw new Error("Failed to create document");
      }

      // Je≈õli jest conversationId, dodaj notatkƒô do konwersacji
      if (conversationId) {
        await supabase.from("messages").insert({
          conversation_id: conversationId,
          role: "system",
          content: `üìÑ Utworzono dokument: "${title}" (typ: ${documentType})`,
          metadata: {
            document_id: newDocument.id,
            action: "document_created",
          },
        });
      }

      return reply.send({
        success: true,
        document: newDocument,
        message: `Dokument "${title}" zosta≈Ç utworzony i zapisany w bazie.`,
      });
    } catch (error) {
      fastify.log.error("Error creating document:", error);
      return reply.status(500).send({
        error: "Failed to create document",
        details: error instanceof Error ? error.message : "Unknown error",
      });
    }
  });

  // POST /api/chat/create-summary - Utw√≥rz podsumowanie dokument√≥w
  fastify.post<{
    Body: {
      query: string;
      documentTypes?: string[];
      conversationId?: string;
    };
  }>("/chat/create-summary", async (request, reply) => {
    try {
      const userId = request.headers["x-user-id"] as string;
      if (!userId) {
        return reply.status(401).send({ error: "Unauthorized" });
      }

      const { query, documentTypes, conversationId } = request.body;

      if (!query) {
        return reply.status(400).send({ error: "Query is required" });
      }

      // Pobierz konfiguracjƒô OpenAI
      const { data: apiConfig } = await supabase
        .from("api_configurations")
        .select("*")
        .eq("user_id", userId)
        .eq("is_active", true)
        .eq("is_default", true)
        .eq("provider", "openai")
        .single();

      let openaiApiKey = process.env.OPENAI_API_KEY;
      let openaiModel = process.env.OPENAI_MODEL || "gpt-4-turbo-preview";
      let openaiBaseUrl: string | undefined = undefined;

      if (apiConfig) {
        openaiApiKey = Buffer.from(
          apiConfig.api_key_encrypted,
          "base64"
        ).toString("utf-8");
        openaiModel = apiConfig.model_name || openaiModel;
        openaiBaseUrl = apiConfig.base_url || undefined;
      }

      if (!openaiApiKey) {
        return reply.status(400).send({
          error: "Brak konfiguracji OpenAI",
        });
      }

      const openai = new OpenAI({
        apiKey: openaiApiKey,
        baseURL: openaiBaseUrl,
      });

      // Generuj embedding dla zapytania
      const embeddingResponse = await openai.embeddings.create({
        model: "text-embedding-3-small",
        input: query,
      });

      const queryEmbedding = embeddingResponse.data[0].embedding;

      // Wyszukaj relevantne dokumenty
      const { data: relevantDocs } = await supabase.rpc(
        "search_processed_documents",
        {
          query_embedding: queryEmbedding,
          match_threshold: 0.6,
          match_count: 10,
          filter_user_id: userId,
          filter_types: documentTypes || null,
        }
      );

      if (!relevantDocs || relevantDocs.length === 0) {
        return reply.status(404).send({
          error: "Nie znaleziono dokument√≥w pasujƒÖcych do zapytania",
        });
      }

      // Przygotuj kontekst dla AI
      let contextMessage = "# DOKUMENTY DO PODSUMOWANIA\n\n";
      relevantDocs.forEach((doc: any, idx: number) => {
        contextMessage += `## Dokument ${idx + 1}: ${doc.title}\n`;
        contextMessage += `Typ: ${doc.document_type}\n`;
        contextMessage += `Tre≈õƒá: ${doc.content}\n\n`;
      });

      // Popro≈õ AI o podsumowanie
      const summaryParams: any = {
        model: openaiModel,
        messages: [
          {
            role: "system",
            content:
              "Jeste≈õ asystentem tworzƒÖcym zwiƒôz≈Çe i merytoryczne podsumowania dokument√≥w. Twoim zadaniem jest przeanalizowaƒá podane dokumenty i stworzyƒá kompleksowe podsumowanie.",
          },
          {
            role: "user",
            content: `${contextMessage}\n\nZapytanie: ${query}\n\nUtw√≥rz szczeg√≥≈Çowe podsumowanie powy≈ºszych dokument√≥w w kontek≈õcie zapytania. Uwzglƒôdnij najwa≈ºniejsze informacje, daty, liczby i fakty.`,
          },
        ],
        temperature: 0.3,
      };

      // U≈ºyj max_completion_tokens dla nowych modeli OpenAI
      if (
        openaiModel.includes("gpt-4o") ||
        openaiModel.includes("gpt-4-turbo") ||
        openaiModel.includes("o1")
      ) {
        summaryParams.max_completion_tokens = 2000;
      } else {
        summaryParams.max_tokens = 2000;
      }

      const completion = await openai.chat.completions.create(summaryParams);

      const summaryContent =
        completion.choices[0]?.message?.content ||
        "Nie uda≈Ço siƒô wygenerowaƒá podsumowania";

      // Zapisz podsumowanie jako nowy dokument
      const summaryTitle = `Podsumowanie: ${query}`;
      const keywords = relevantDocs.map((doc: any) => doc.document_type);

      const embeddingSummary = await openai.embeddings.create({
        model: "text-embedding-3-small",
        input: summaryContent,
      });

      const { data: summaryDoc } = await supabase
        .from("processed_documents")
        .insert({
          user_id: userId,
          document_type: "article",
          title: summaryTitle,
          content: summaryContent,
          summary: summaryContent.substring(0, 200) + "...",
          keywords: Array.from(new Set(keywords)),
          embedding: embeddingSummary.data[0].embedding,
          metadata: {
            created_by: "ai_assistant",
            source_documents: relevantDocs.map((d: any) => d.id),
            query: query,
            conversation_id: conversationId,
            created_at: new Date().toISOString(),
          },
        })
        .select()
        .single();

      return reply.send({
        success: true,
        summary: summaryContent,
        document: summaryDoc,
        sourceDocuments: relevantDocs.length,
      });
    } catch (error) {
      fastify.log.error("Error creating summary:", error);
      return reply.status(500).send({
        error: "Failed to create summary",
        details: error instanceof Error ? error.message : "Unknown error",
      });
    }
  });

  // POST /api/fetch-models - Pobierz listƒô modeli z API providera
  fastify.post("/api/fetch-models", async (request, reply) => {
    try {
      const { provider, apiKey, baseUrl } = request.body as {
        provider: string;
        apiKey: string;
        baseUrl?: string;
      };

      if (!provider || !apiKey) {
        return reply.status(400).send({
          error: "Provider i klucz API sƒÖ wymagane",
        });
      }

      // Domy≈õlne URL dla provider√≥w - tylko OpenAI API compatible
      const providerBaseUrls: Record<string, string> = {
        openai: "https://api.openai.com/v1",
        local: "http://localhost:11434/v1", // Ollama default
        other: "", // Custom endpoint
      };

      const finalBaseUrl = baseUrl || providerBaseUrls[provider];

      if (!finalBaseUrl) {
        return reply.status(400).send({
          error: "Nieznany provider lub brak URL API",
        });
      }

      // Pobierz listƒô modeli z API
      const modelsUrl = `${finalBaseUrl.replace(/\/$/, "")}/models`;

      console.log(`[FetchModels] Fetching from: ${modelsUrl}`);

      // R√≥≈ºne providery u≈ºywajƒÖ r√≥≈ºnych metod autoryzacji
      const headers: Record<string, string> = {
        "Content-Type": "application/json",
      };

      // Google Gemini u≈ºywa x-goog-api-key dla native API
      if (provider === "google" && !finalBaseUrl.includes("/openai")) {
        headers["x-goog-api-key"] = apiKey;
      } else {
        // Pozostali providerzy u≈ºywajƒÖ Bearer token
        headers["Authorization"] = `Bearer ${apiKey}`;
      }

      const response = await fetch(modelsUrl, {
        method: "GET",
        headers: headers,
      });

      if (!response.ok) {
        const errorText = await response.text();
        console.error(`[FetchModels] Error: ${response.status} - ${errorText}`);
        return reply.status(response.status).send({
          error: `B≈ÇƒÖd API: ${response.status}`,
          details: errorText,
        });
      }

      const data = await response.json();

      // R√≥≈ºne providery zwracajƒÖ r√≥≈ºne formaty
      let models: { id: string; name?: string; owned_by?: string }[] = [];

      // Google Gemini: { models: [{ name: "models/gemini-...", displayName: "..." }] }
      if (data.models && Array.isArray(data.models)) {
        models = data.models.map(
          (m: {
            name?: string;
            displayName?: string;
            description?: string;
          }) => ({
            id: m.name?.replace("models/", "") || m.displayName || "",
            name: m.displayName || m.name?.replace("models/", "") || "",
            owned_by: "google",
          })
        );
      }
      // OpenAI: { data: [{ id, object, ... }] }
      else if (data.data && Array.isArray(data.data)) {
        models = data.data.map(
          (m: { id: string; name?: string; owned_by?: string }) => ({
            id: m.id,
            name: m.name || m.id,
            owned_by: m.owned_by,
          })
        );
      }
      // Inne formaty: bezpo≈õrednia tablica
      else if (Array.isArray(data)) {
        models = data.map(
          (m: { id?: string; name?: string; model?: string }) => ({
            id: m.id || m.model || m.name || "",
            name: m.name || m.id || m.model || "",
          })
        );
      }

      // Filtruj i sortuj modele - preferuj chat/completion modele
      const chatModels = models.filter((m) => {
        const id = m.id.toLowerCase();
        // Wyklucz modele embedding, whisper, dall-e, tts
        return (
          !id.includes("embedding") &&
          !id.includes("whisper") &&
          !id.includes("dall-e") &&
          !id.includes("tts") &&
          !id.includes("moderation")
        );
      });

      // Sortuj - najnowsze/najlepsze na g√≥rze
      chatModels.sort((a, b) => {
        const aId = a.id.toLowerCase();
        const bId = b.id.toLowerCase();
        // Priorytet dla najnowszych modeli
        if (aId.includes("gpt-4o") && !bId.includes("gpt-4o")) return -1;
        if (bId.includes("gpt-4o") && !aId.includes("gpt-4o")) return 1;
        if (aId.includes("gpt-4") && !bId.includes("gpt-4")) return -1;
        if (bId.includes("gpt-4") && !aId.includes("gpt-4")) return 1;
        if (aId.includes("gemini-2") && !bId.includes("gemini-2")) return -1;
        if (bId.includes("gemini-2") && !aId.includes("gemini-2")) return 1;
        if (aId.includes("claude-3") && !bId.includes("claude-3")) return -1;
        if (bId.includes("claude-3") && !aId.includes("claude-3")) return 1;
        return a.id.localeCompare(b.id);
      });

      console.log(`[FetchModels] Found ${chatModels.length} chat models`);

      return reply.send({
        success: true,
        models: chatModels,
        total: chatModels.length,
      });
    } catch (error) {
      console.error("[FetchModels] Error:", error);
      return reply.status(500).send({
        error: "Nie uda≈Ço siƒô pobraƒá listy modeli",
        details: error instanceof Error ? error.message : "Unknown error",
      });
    }
  });
};
