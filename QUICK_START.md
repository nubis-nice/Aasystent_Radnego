# üöÄ Quick Start - ~~bez~~RADNY

## ‚úÖ Checklist uruchomienia

### Krok 1: Przygotowanie ≈õrodowiska ‚úÖ (GOTOWE)

- ‚úÖ Migracje SQL ju≈º w Supabase
- ‚úÖ Kod zrefaktorowany i gotowy

### Krok 2: Konfiguracja zmiennych ≈õrodowiskowych

**Backend** - Utw√≥rz `apps/api/.env`:

```env
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key
API_PORT=3001
LOG_LEVEL=info
FRONTEND_URL=http://localhost:3000
```

**Frontend** - Utw√≥rz `apps/frontend/.env.local`:

```env
NEXT_PUBLIC_SUPABASE_URL=https://your-project.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=your-anon-key
NEXT_PUBLIC_API_URL=http://localhost:3001
```

### Krok 3: Instalacja i build

```bash
# Zainstaluj zale≈ºno≈õci
npm install

# Zbuduj shared package (WA≈ªNE!)
npm run build:shared
```

### Krok 4: Uruchomienie aplikacji

```bash
# Uruchom wszystkie serwisy
npm run dev

# Aplikacja dostƒôpna:
# - Frontend: http://localhost:3000
# - Backend API: http://localhost:3001
```

### Krok 5: Pierwsza konfiguracja w UI

1. **Otw√≥rz** `http://localhost:3000`
2. **Zaloguj siƒô** przez Google OAuth
3. **Ustawienia ‚Üí Konfiguracja API**:
   - Dodaj konfiguracjƒô OpenAI
   - API Key: `sk-...`
   - Model: `gpt-4` lub `gpt-3.5-turbo`
   - Embedding Model: `text-embedding-3-small`
   - ‚úÖ Ustaw jako domy≈õlny

### Krok 6: Dodaj ≈∫r√≥d≈Ça danych

**Ustawienia ‚Üí ≈πr√≥d≈Ça Danych** ‚Üí Dodaj ≈∫r√≥d≈Ço:

**Przyk≈Çad 1: BIP Gminy**

- Nazwa: `BIP Gminy Drawno`
- URL: `https://bip.drawno.pl`
- Typ: `BIP - Biuletyn Informacji Publicznej`
- Metoda: `Scraping (Web)`
- ‚úÖ Zapisz

**Przyk≈Çad 2: ISAP**

- Nazwa: `ISAP - Akty Prawne`
- URL: `https://isap.sejm.gov.pl`
- Typ: `ISAP - Akty prawne`
- Metoda: `Scraping (Web)`
- ‚úÖ Zapisz

### Krok 7: Uruchom scraping

1. W li≈õcie ≈∫r√≥de≈Ç kliknij **Scrapuj** przy wybranym ≈∫r√≥dle
2. Poczekaj na zako≈Ñczenie (sprawd≈∫ logi w terminalu)
3. Sprawd≈∫ statystyki - powinny pojawiƒá siƒô pobrane dokumenty

### Krok 8: Testuj analizy

**Przejd≈∫ do `/analysis`** i przetestuj:

**Wyszukiwanie:**

- Query: `bud≈ºet gminy`
- Tryb: `Hybrydowe`
- Kliknij **Szukaj**

**Analiza prawna:**

- Pytanie: `Czy uchwa≈Ça bud≈ºetowa jest zgodna z ustawƒÖ o finansach publicznych?`
- Typ: `Analiza legalno≈õci`
- Kliknij **Analizuj**

## üîç Weryfikacja dzia≈Çania

### 1. Sprawd≈∫ health endpoint

```bash
curl http://localhost:3001/health
# Powinno zwr√≥ciƒá: {"status":"ok","timestamp":"...","version":"1.0.0"}
```

### 2. Sprawd≈∫ czy shared package siƒô zbudowa≈Ç

```bash
ls packages/shared/dist/
# Powinny byƒá pliki: index.js, index.d.ts, types/
```

### 3. Sprawd≈∫ logi API

Terminal z `npm run dev` powinien pokazywaƒá:

```
[API] Server listening at http://0.0.0.0:3001
[Frontend] Ready on http://localhost:3000
```

### 4. Sprawd≈∫ bazƒô danych

W Supabase Dashboard ‚Üí SQL Editor:

```sql
-- Sprawd≈∫ nowe kolumny
SELECT column_name FROM information_schema.columns
WHERE table_name = 'data_sources'
AND column_name IN ('fetch_method', 'api_config', 'category');

-- Sprawd≈∫ funkcje RPC
SELECT routine_name FROM information_schema.routines
WHERE routine_name IN ('match_documents', 'hybrid_search');
```

## ‚ö†Ô∏è RozwiƒÖzywanie problem√≥w

### Problem: "Cannot find module '@shared/types/data-sources-api'"

**RozwiƒÖzanie:**

```bash
npm run build:shared
```

### Problem: "OpenAI API configuration not found"

**RozwiƒÖzanie:**

1. Zaloguj siƒô do aplikacji
2. Ustawienia ‚Üí Konfiguracja API
3. Dodaj konfiguracjƒô OpenAI i ustaw jako domy≈õlnƒÖ

### Problem: Scraping nie dzia≈Ça

**RozwiƒÖzanie:**

1. Sprawd≈∫ logi API w terminalu
2. Sprawd≈∫ czy URL ≈∫r√≥d≈Ça jest dostƒôpny
3. Sprawd≈∫ konfiguracjƒô selektor√≥w CSS

### Problem: Semantic search nie zwraca wynik√≥w

**RozwiƒÖzanie:**

1. Upewnij siƒô, ≈ºe dokumenty majƒÖ embeddingi (`embedding IS NOT NULL`)
2. Sprawd≈∫ czy OpenAI API key jest poprawny
3. Sprawd≈∫ czy funkcja `match_documents` istnieje w bazie

## üìö Nastƒôpne kroki

1. ‚úÖ **Dodaj wiƒôcej ≈∫r√≥de≈Ç** - BIP, RIO, ISAP
2. ‚úÖ **Uruchom scraping** dla wszystkich ≈∫r√≥de≈Ç
3. ‚úÖ **Przetestuj analizy** - wyszukiwanie i reasoning
4. ‚úÖ **Skonfiguruj harmonogramy** - automatyczne scrapowanie
5. ‚úÖ **Monitoruj logi** - sprawdzaj b≈Çƒôdy i ostrze≈ºenia

## üéØ Kluczowe endpointy

- `GET /health` - status API
- `GET /api/data-sources` - lista ≈∫r√≥de≈Ç
- `POST /api/data-sources/:id/scrape` - uruchom scraping
- `POST /api/legal/search` - wyszukiwanie prawne
- `POST /api/legal/reasoning` - analiza prawna
- `POST /api/legal/budget-analysis` - analiza bud≈ºetowa

## üìñ Dokumentacja

- [`README.md`](README.md) - g≈Ç√≥wny README
- [`docs/INSTRUKCJA_URUCHOMIENIA_WINSDURF.md`](docs/INSTRUKCJA_URUCHOMIENIA_WINSDURF.md) - szczeg√≥≈Çowa instrukcja
- [`docs/REFACTORING_SUMMARY_2026_01_09.md`](docs/REFACTORING_SUMMARY_2026_01_09.md) - podsumowanie refactoringu
- [`docs/architecture.md`](docs/architecture.md) - architektura systemu

---

**~~bez~~RADNY gotowy! Powodzenia! üöÄ**

---

**Licencja**: MIT | **Data aktualizacji**: 2026-01-25
